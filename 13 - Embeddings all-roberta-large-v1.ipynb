{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d4714b6-6152-45c6-a7cc-387958d3f68c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:167: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
       "  original_result = python_builtin_import(name, globals, locals, fromlist, level)\n",
       "Downloaded 2097 articles.\n",
       "modules.json:   0%|          | 0.00/349 [00:00&lt;?, ?B/s]\n",
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00&lt;?, ?B/s]\n",
       "README.md:   0%|          | 0.00/9.89k [00:00&lt;?, ?B/s]\n",
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00&lt;?, ?B/s]\n",
       "/databricks/python/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
       "  warnings.warn(\n",
       "config.json:   0%|          | 0.00/650 [00:00&lt;?, ?B/s]\n",
       "model.safetensors:   0%|          | 0.00/1.42G [00:00&lt;?, ?B/s]\n",
       "tokenizer_config.json:   0%|          | 0.00/328 [00:00&lt;?, ?B/s]\n",
       "vocab.json:   0%|          | 0.00/798k [00:00&lt;?, ?B/s]\n",
       "merges.txt:   0%|          | 0.00/456k [00:00&lt;?, ?B/s]\n",
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00&lt;?, ?B/s]\n",
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00&lt;?, ?B/s]\n",
       "1_Pooling/config.json:   0%|          | 0.00/191 [00:00&lt;?, ?B/s]\n",
       "Processed 1 articles\n",
       "Processed 101 articles\n",
       "Processed 201 articles\n",
       "Processed 301 articles\n",
       "Processed 401 articles\n",
       "Processed 501 articles\n",
       "Processed 601 articles\n",
       "Processed 701 articles\n",
       "Processed 801 articles\n",
       "Processed 901 articles\n",
       "Processed 1001 articles\n",
       "Processed 1101 articles\n",
       "Processed 1201 articles\n",
       "Processed 1301 articles\n",
       "Processed 1401 articles\n",
       "Processed 1501 articles\n",
       "Processed 1601 articles\n",
       "Processed 1701 articles\n",
       "Processed 1801 articles\n",
       "Processed 1901 articles\n",
       "Processed 2001 articles\n",
       "Uploaded embeddings for 2097 articles to articles_embeddings_sbert_cls_ALL_roberta.json.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:167: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  original_result = python_builtin_import(name, globals, locals, fromlist, level)\nDownloaded 2097 articles.\nmodules.json:   0%|          | 0.00/349 [00:00&lt;?, ?B/s]\nconfig_sentence_transformers.json:   0%|          | 0.00/116 [00:00&lt;?, ?B/s]\nREADME.md:   0%|          | 0.00/9.89k [00:00&lt;?, ?B/s]\nsentence_bert_config.json:   0%|          | 0.00/53.0 [00:00&lt;?, ?B/s]\n/databricks/python/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nconfig.json:   0%|          | 0.00/650 [00:00&lt;?, ?B/s]\nmodel.safetensors:   0%|          | 0.00/1.42G [00:00&lt;?, ?B/s]\ntokenizer_config.json:   0%|          | 0.00/328 [00:00&lt;?, ?B/s]\nvocab.json:   0%|          | 0.00/798k [00:00&lt;?, ?B/s]\nmerges.txt:   0%|          | 0.00/456k [00:00&lt;?, ?B/s]\ntokenizer.json:   0%|          | 0.00/1.36M [00:00&lt;?, ?B/s]\nspecial_tokens_map.json:   0%|          | 0.00/239 [00:00&lt;?, ?B/s]\n1_Pooling/config.json:   0%|          | 0.00/191 [00:00&lt;?, ?B/s]\nProcessed 1 articles\nProcessed 101 articles\nProcessed 201 articles\nProcessed 301 articles\nProcessed 401 articles\nProcessed 501 articles\nProcessed 601 articles\nProcessed 701 articles\nProcessed 801 articles\nProcessed 901 articles\nProcessed 1001 articles\nProcessed 1101 articles\nProcessed 1201 articles\nProcessed 1301 articles\nProcessed 1401 articles\nProcessed 1501 articles\nProcessed 1601 articles\nProcessed 1701 articles\nProcessed 1801 articles\nProcessed 1901 articles\nProcessed 2001 articles\nUploaded embeddings for 2097 articles to articles_embeddings_sbert_cls_ALL_roberta.json.\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Conectar al Blob Storage\n",
    "blob_service_client = BlobServiceClient.from_connection_string(\"***************************************\")\n",
    "container_name = \"*****\"\n",
    "input_blob_name = \"articles.json\"\n",
    "embeddings_blob_name = \"articles_embeddings_sbert_cls_ALL_roberta.json\"\n",
    "input_blob_client = blob_service_client.get_blob_client(container_name, input_blob_name)\n",
    "embeddings_blob_client = blob_service_client.get_blob_client(container_name, embeddings_blob_name)\n",
    "\n",
    "# Descargar el archivo JSON\n",
    "try:\n",
    "    downloaded_blob = input_blob_client.download_blob().readall()\n",
    "    articles_data = json.loads(downloaded_blob)\n",
    "    print(f\"Downloaded {len(articles_data)} articles.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al descargar el archivo JSON: {e}\")\n",
    "    raise\n",
    "\n",
    "# Cargar el modelo all-roberta-large-v1\n",
    "model_name = \"all-roberta-large-v1\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Función para generar embeddings [CLS]\n",
    "def generate_cls_embedding(text):\n",
    "    embedding = model.encode(text, convert_to_tensor=True)\n",
    "    return embedding.cpu().tolist()\n",
    "\n",
    "# Procesar todos los artículos y generar embeddings\n",
    "embeddings_data = []\n",
    "\n",
    "for idx, article in enumerate(articles_data):\n",
    "    try:\n",
    "        content_id = article['contentId']\n",
    "        text = article['text']  \n",
    "        embeddings = generate_cls_embedding(text)\n",
    "        embeddings_data.append({\n",
    "            'contentId': content_id,\n",
    "            'embeddings': embeddings\n",
    "        })\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Processed {idx + 1} articles\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar el artículo {content_id}: {e}\")\n",
    "\n",
    "# Guardar los embeddings generados en un archivo JSON\n",
    "try:\n",
    "    embeddings_json = json.dumps(embeddings_data, ensure_ascii=False, indent=4)\n",
    "    embeddings_blob_client.upload_blob(embeddings_json, overwrite=True)\n",
    "    print(f\"Uploaded embeddings for {len(embeddings_data)} articles to {embeddings_blob_name}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al subir los embeddings al Blob Storage: {e}\")\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "13 - Embeddings all-roberta-large-v1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
