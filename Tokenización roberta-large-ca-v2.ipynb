{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a463e1a-3a6a-4b07-8900-182a28189bbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Tokens de noticias generados y guardados en Azure Blob Storage. Total de fragmentos tokenizados: 4933\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Tokens de noticias generados y guardados en Azure Blob Storage. Total de fragmentos tokenizados: 4933\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# Configuración de Azure Blob Storage\n",
    "storage_account_name = \"*************\"\n",
    "container_name = \"*************\"\n",
    "storage_account_key = \"*************\"\n",
    "\n",
    "# Crear BlobServiceClient\n",
    "blob_service_client = BlobServiceClient(account_url=f\"https://{storage_account_name}.blob.core.windows.net\", credential=storage_account_key)\n",
    "\n",
    "# Descargar el archivo de artículos del Blob Storage\n",
    "try:\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=\"articles.json\")\n",
    "    articles_data = json.loads(blob_client.download_blob().readall())\n",
    "except Exception as e:\n",
    "    print(f\"Error al descargar el archivo de artículos: {e}\")\n",
    "    raise\n",
    "\n",
    "# Inicializar el tokenizador de Roberta Large CA\n",
    "tokenizer = RobertaTokenizer.from_pretrained('projecte-aina/roberta-large-ca-v2')\n",
    "\n",
    "# Función para dividir el texto en fragmentos\n",
    "def split_text(text, max_length=512):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [' '.join(tokens[i:i + max_length]) for i in range(0, len(tokens), max_length)]\n",
    "\n",
    "# Tokenizar las noticias\n",
    "def tokenize_articles(articles):\n",
    "    tokenized_articles = []\n",
    "    for article in articles:\n",
    "        text_fragments = split_text(article['text'])\n",
    "        for idx, fragment in enumerate(text_fragments):\n",
    "            tokens = tokenizer(\n",
    "                fragment, \n",
    "                padding='max_length', \n",
    "                truncation=True, \n",
    "                max_length=512,  # Máximo de tokens para RoBERTa\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            tokenized_articles.append({\n",
    "                'contentId': article['contentId'],  # Mantener contentId original\n",
    "                'content_token_id': f\"{article['contentId']}_{idx}\",  # Identificar fragmentos\n",
    "                'input_ids': tokens['input_ids'].tolist(),\n",
    "                'attention_mask': tokens['attention_mask'].tolist(),\n",
    "                'topicName': article['topicName']\n",
    "            })\n",
    "    return tokenized_articles\n",
    "\n",
    "try:\n",
    "    tokenized_articles = tokenize_articles(articles_data)\n",
    "    # Convertir a JSON\n",
    "    tokenized_articles_json = json.dumps(tokenized_articles, ensure_ascii=False, indent=4)\n",
    "except Exception as e:\n",
    "    print(f\"Error durante la tokenización: {e}\")\n",
    "    raise\n",
    "\n",
    "# Crear BlobClient para los tokens\n",
    "try:\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=\"articles_tokens_roberta_CA.json\")\n",
    "    # Subir los tokens al Blob Storage\n",
    "    blob_client.upload_blob(tokenized_articles_json, blob_type=\"BlockBlob\", overwrite=True)\n",
    "    print(f\"Tokens de noticias generados y guardados en Azure Blob Storage. Total de fragmentos tokenizados: {len(tokenized_articles)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al subir los tokens al Blob Storage: {e}\")\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BO Tokenización roberta-large-ca-v2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
