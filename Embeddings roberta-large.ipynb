{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78906c25-689d-452c-93a4-9a98cfe5409a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Downloaded 7414 tokenized articles.\n",
       "model.safetensors:   0%|          | 0.00/1.42G [00:00&lt;?, ?B/s]\n",
       "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: [&#39;roberta.pooler.dense.bias&#39;, &#39;roberta.pooler.dense.weight&#39;]\n",
       "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
       "Processed 1 articles\n",
       "Processed 101 articles\n",
       "Processed 201 articles\n",
       "Processed 301 articles\n",
       "Processed 401 articles\n",
       "Processed 501 articles\n",
       "Processed 601 articles\n",
       "Processed 701 articles\n",
       "Processed 801 articles\n",
       "Processed 901 articles\n",
       "Processed 1001 articles\n",
       "Processed 1101 articles\n",
       "Processed 1201 articles\n",
       "Processed 1301 articles\n",
       "Processed 1401 articles\n",
       "Processed 1501 articles\n",
       "Processed 1601 articles\n",
       "Processed 1701 articles\n",
       "Processed 1801 articles\n",
       "Processed 1901 articles\n",
       "Processed 2001 articles\n",
       "Processed 2101 articles\n",
       "Processed 2201 articles\n",
       "Processed 2301 articles\n",
       "Processed 2401 articles\n",
       "Processed 2501 articles\n",
       "Processed 2601 articles\n",
       "Processed 2701 articles\n",
       "Processed 2801 articles\n",
       "Processed 2901 articles\n",
       "Processed 3001 articles\n",
       "Processed 3101 articles\n",
       "Processed 3201 articles\n",
       "Processed 3301 articles\n",
       "Processed 3401 articles\n",
       "Processed 3501 articles\n",
       "Processed 3601 articles\n",
       "Processed 3701 articles\n",
       "Processed 3801 articles\n",
       "Processed 3901 articles\n",
       "Processed 4001 articles\n",
       "Processed 4101 articles\n",
       "Processed 4201 articles\n",
       "Processed 4301 articles\n",
       "Processed 4401 articles\n",
       "Processed 4501 articles\n",
       "Processed 4601 articles\n",
       "Processed 4701 articles\n",
       "Processed 4801 articles\n",
       "Processed 4901 articles\n",
       "Processed 5001 articles\n",
       "Processed 5101 articles\n",
       "Processed 5201 articles\n",
       "Processed 5301 articles\n",
       "Processed 5401 articles\n",
       "Processed 5501 articles\n",
       "Processed 5601 articles\n",
       "Processed 5701 articles\n",
       "Processed 5801 articles\n",
       "Processed 5901 articles\n",
       "Processed 6001 articles\n",
       "Processed 6101 articles\n",
       "Processed 6201 articles\n",
       "Processed 6301 articles\n",
       "Processed 6401 articles\n",
       "Processed 6501 articles\n",
       "Processed 6601 articles\n",
       "Processed 6701 articles\n",
       "Processed 6801 articles\n",
       "Processed 6901 articles\n",
       "Processed 7001 articles\n",
       "Processed 7101 articles\n",
       "Processed 7201 articles\n",
       "Processed 7301 articles\n",
       "Processed 7401 articles\n",
       "Uploaded embeddings for 7414 fragments to articles_fragments_embeddings_roberta.json.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Downloaded 7414 tokenized articles.\nmodel.safetensors:   0%|          | 0.00/1.42G [00:00&lt;?, ?B/s]\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: [&#39;roberta.pooler.dense.bias&#39;, &#39;roberta.pooler.dense.weight&#39;]\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nProcessed 1 articles\nProcessed 101 articles\nProcessed 201 articles\nProcessed 301 articles\nProcessed 401 articles\nProcessed 501 articles\nProcessed 601 articles\nProcessed 701 articles\nProcessed 801 articles\nProcessed 901 articles\nProcessed 1001 articles\nProcessed 1101 articles\nProcessed 1201 articles\nProcessed 1301 articles\nProcessed 1401 articles\nProcessed 1501 articles\nProcessed 1601 articles\nProcessed 1701 articles\nProcessed 1801 articles\nProcessed 1901 articles\nProcessed 2001 articles\nProcessed 2101 articles\nProcessed 2201 articles\nProcessed 2301 articles\nProcessed 2401 articles\nProcessed 2501 articles\nProcessed 2601 articles\nProcessed 2701 articles\nProcessed 2801 articles\nProcessed 2901 articles\nProcessed 3001 articles\nProcessed 3101 articles\nProcessed 3201 articles\nProcessed 3301 articles\nProcessed 3401 articles\nProcessed 3501 articles\nProcessed 3601 articles\nProcessed 3701 articles\nProcessed 3801 articles\nProcessed 3901 articles\nProcessed 4001 articles\nProcessed 4101 articles\nProcessed 4201 articles\nProcessed 4301 articles\nProcessed 4401 articles\nProcessed 4501 articles\nProcessed 4601 articles\nProcessed 4701 articles\nProcessed 4801 articles\nProcessed 4901 articles\nProcessed 5001 articles\nProcessed 5101 articles\nProcessed 5201 articles\nProcessed 5301 articles\nProcessed 5401 articles\nProcessed 5501 articles\nProcessed 5601 articles\nProcessed 5701 articles\nProcessed 5801 articles\nProcessed 5901 articles\nProcessed 6001 articles\nProcessed 6101 articles\nProcessed 6201 articles\nProcessed 6301 articles\nProcessed 6401 articles\nProcessed 6501 articles\nProcessed 6601 articles\nProcessed 6701 articles\nProcessed 6801 articles\nProcessed 6901 articles\nProcessed 7001 articles\nProcessed 7101 articles\nProcessed 7201 articles\nProcessed 7301 articles\nProcessed 7401 articles\nUploaded embeddings for 7414 fragments to articles_fragments_embeddings_roberta.json.\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import RobertaModel\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "#Embeddings de los fragmentos de Tokens\n",
    "\n",
    "# Conectar al Blob Storage\n",
    "blob_service_client = BlobServiceClient.from_connection_string(\"*************\")\n",
    "container_name = \"*************\"\n",
    "input_blob_name = \"articles_tokens_roberta.json\"\n",
    "embeddings_blob_name = \"articles_fragments_embeddings_roberta.json\"\n",
    "input_blob_client = blob_service_client.get_blob_client(container_name, input_blob_name)\n",
    "embeddings_blob_client = blob_service_client.get_blob_client(container_name, embeddings_blob_name)\n",
    "\n",
    "# Descargar el archivo JSON\n",
    "try:\n",
    "    downloaded_blob = input_blob_client.download_blob().readall()\n",
    "    tokenized_articles = json.loads(downloaded_blob)\n",
    "    print(f\"Downloaded {len(tokenized_articles)} tokenized articles.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al descargar el archivo JSON: {e}\")\n",
    "    raise\n",
    "\n",
    "# Cargar modelo y tokenizer\n",
    "model_name = \"roberta-large\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RobertaModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Función para generar embeddings\n",
    "def generate_embeddings(tokens):\n",
    "    input_ids = torch.tensor(tokens['input_ids'], dtype=torch.long).to(device)\n",
    "    attention_mask = torch.tensor(tokens['attention_mask'], dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    return last_hidden_state.mean(dim=1).squeeze(0).tolist()\n",
    "\n",
    "# Procesar todos los artículos y generar embeddings\n",
    "embeddings_data = []\n",
    "\n",
    "for idx, article in enumerate(tokenized_articles):\n",
    "    try:\n",
    "        content_id = article['contentId']\n",
    "        content_token_id = article['content_token_id']\n",
    "        tokens = {'input_ids': article['input_ids'], 'attention_mask': article['attention_mask']}\n",
    "        embeddings = generate_embeddings(tokens)\n",
    "        embeddings_data.append({\n",
    "            'contentId': content_id,\n",
    "            'content_token_id': content_token_id,\n",
    "            'embeddings': embeddings\n",
    "        })\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Processed {idx + 1} articles\")\n",
    "\n",
    "        # Liberar memoria de GPU si es necesario\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar el artículo {content_id}: {e}\")\n",
    "\n",
    "# Guardar los embeddings generados en un archivo JSON\n",
    "try:\n",
    "    embeddings_json = json.dumps(embeddings_data)\n",
    "    embeddings_blob_client.upload_blob(embeddings_json, overwrite=True)\n",
    "    print(f\"Uploaded embeddings for {len(embeddings_data)} fragments to {embeddings_blob_name}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al subir los embeddings al Blob Storage: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "082e5c96-c7a0-497d-a65f-45724962227a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Downloaded 7414 embeddings fragments.\n",
       "Uploaded meanpooled embeddings for 2067 articles to articles_embeddings_meanpooled_roberta.json.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Downloaded 7414 embeddings fragments.\nUploaded meanpooled embeddings for 2067 articles to articles_embeddings_meanpooled_roberta.json.\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Técnica MEAN Pooling\n",
    "\n",
    "# Conectar al Blob Storage\n",
    "blob_service_client = BlobServiceClient.from_connection_string(\"***************\")\n",
    "container_name = \"***************\"\n",
    "embeddings_blob_name = \"articles_fragments_embeddings_roberta.json\"\n",
    "output_blob_name = \"articles_embeddings_meanpooled_roberta.json\"  # Cambiar el nombre del archivo de salida\n",
    "embeddings_blob_client = blob_service_client.get_blob_client(container_name, embeddings_blob_name)\n",
    "output_blob_client = blob_service_client.get_blob_client(container_name, output_blob_name)\n",
    "\n",
    "# Descargar el archivo JSON de embeddings\n",
    "try:\n",
    "    downloaded_blob = embeddings_blob_client.download_blob().readall()\n",
    "    embeddings_data = json.loads(downloaded_blob)\n",
    "    print(f\"Downloaded {len(embeddings_data)} embeddings fragments.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al descargar el archivo JSON: {e}\")\n",
    "    raise\n",
    "\n",
    "# Agrupar embeddings de fragmentos del mismo artículo\n",
    "article_embeddings = {}\n",
    "\n",
    "for item in embeddings_data:\n",
    "    content_id = item['contentId']\n",
    "    embeddings = item['embeddings']\n",
    "    \n",
    "    if content_id in article_embeddings:\n",
    "        article_embeddings[content_id].append(embeddings)\n",
    "    else:\n",
    "        article_embeddings[content_id] = [embeddings]\n",
    "\n",
    "meanpooled_embeddings_data = []\n",
    "\n",
    "for content_id, embeddings_list in article_embeddings.items():\n",
    "    # Convertir lista de embeddings a tensor\n",
    "    embeddings_tensor = torch.tensor(embeddings_list)\n",
    "    # Aplicar mean pooling a lo largo de la primera dimensión\n",
    "    meanpooled_embeddings = torch.mean(embeddings_tensor, dim=0).tolist()\n",
    "    meanpooled_embeddings_data.append({\n",
    "        'contentId': content_id,\n",
    "        'embeddings': meanpooled_embeddings\n",
    "    })\n",
    "\n",
    "# Guardar los embeddings meanpooled en un archivo JSON\n",
    "try:\n",
    "    meanpooled_embeddings_json = json.dumps(meanpooled_embeddings_data)\n",
    "    output_blob_client.upload_blob(meanpooled_embeddings_json, blob_type=\"BlockBlob\", overwrite=True)\n",
    "    print(f\"Uploaded meanpooled embeddings for {len(meanpooled_embeddings_data)} articles to {output_blob_name}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al subir los embeddings meanpooled al Blob Storage: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2067215-7ae0-4b5f-946f-10e855a0c7ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Downloaded 8785 embeddings fragments.\n",
       "Uploaded maxpooled embeddings for 2293 articles to articles_embeddings_maxpooled_roberta.json.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Downloaded 8785 embeddings fragments.\nUploaded maxpooled embeddings for 2293 articles to articles_embeddings_maxpooled_roberta.json.\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Técnica MAX Pooling\n",
    "\n",
    "# Conectar al Blob Storage\n",
    "blob_service_client = BlobServiceClient.from_connection_string(\"*****************\")\n",
    "container_name = \"*****************\"\n",
    "embeddings_blob_name = \"articles_fragments_embeddings_roberta.json\"\n",
    "output_blob_name = \"articles_embeddings_maxpooled_roberta.json\"\n",
    "embeddings_blob_client = blob_service_client.get_blob_client(container_name, embeddings_blob_name)\n",
    "output_blob_client = blob_service_client.get_blob_client(container_name, output_blob_name)\n",
    "\n",
    "# Descargar el archivo JSON de embeddings\n",
    "try:\n",
    "    downloaded_blob = embeddings_blob_client.download_blob().readall()\n",
    "    embeddings_data = json.loads(downloaded_blob)\n",
    "    print(f\"Downloaded {len(embeddings_data)} embeddings fragments.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al descargar el archivo JSON: {e}\")\n",
    "    raise\n",
    "\n",
    "# Agrupar embeddings de fragmentos del mismo artículo\n",
    "article_embeddings = {}\n",
    "\n",
    "for item in embeddings_data:\n",
    "    content_id = item['contentId']\n",
    "    embeddings = item['embeddings']\n",
    "    \n",
    "    if content_id in article_embeddings:\n",
    "        article_embeddings[content_id].append(embeddings)\n",
    "    else:\n",
    "        article_embeddings[content_id] = [embeddings]\n",
    "\n",
    "maxpooled_embeddings_data = []\n",
    "\n",
    "for content_id, embeddings_list in article_embeddings.items():\n",
    "    # Convertir lista de embeddings a tensor\n",
    "    embeddings_tensor = torch.tensor(embeddings_list)\n",
    "    # Aplicar max pooling a lo largo de la primera dimensión\n",
    "    maxpooled_embeddings = torch.max(embeddings_tensor, dim=0).values.tolist()\n",
    "    maxpooled_embeddings_data.append({\n",
    "        'contentId': content_id,\n",
    "        'embeddings': maxpooled_embeddings\n",
    "    })\n",
    "\n",
    "# Guardar los embeddings maxpooled en un archivo JSON\n",
    "try:\n",
    "    maxpooled_embeddings_json = json.dumps(maxpooled_embeddings_data)\n",
    "    output_blob_client.upload_blob(maxpooled_embeddings_json, blob_type=\"BlockBlob\", overwrite=True)\n",
    "    print(f\"Uploaded maxpooled embeddings for {len(maxpooled_embeddings_data)} articles to {output_blob_name}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al subir los embeddings maxpooled al Blob Storage: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbeb7c46-1403-42b8-8271-04cf6c0f7480",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Downloaded 7414 embeddings fragments.\n",
       "Uploaded concatenated embeddings for 2067 articles to articles_embeddings_concatenated_roberta.json.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Downloaded 7414 embeddings fragments.\nUploaded concatenated embeddings for 2067 articles to articles_embeddings_concatenated_roberta.json.\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Técnica Concatenación\n",
    "\n",
    "# Conectar al Blob Storage\n",
    "blob_service_client = BlobServiceClient.from_connection_string(\"*************\")\n",
    "container_name = \"*************\"\n",
    "embeddings_blob_name = \"articles_fragments_embeddings_roberta.json\"\n",
    "output_blob_name = \"articles_embeddings_concatenated_roberta.json\"\n",
    "embeddings_blob_client = blob_service_client.get_blob_client(container_name, embeddings_blob_name)\n",
    "output_blob_client = blob_service_client.get_blob_client(container_name, output_blob_name)\n",
    "\n",
    "# Descargar el archivo JSON de embeddings\n",
    "try:\n",
    "    downloaded_blob = embeddings_blob_client.download_blob().readall()\n",
    "    embeddings_data = json.loads(downloaded_blob)\n",
    "    print(f\"Downloaded {len(embeddings_data)} embeddings fragments.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al descargar el archivo JSON: {e}\")\n",
    "    raise\n",
    "\n",
    "# Agrupar embeddings de fragmentos del mismo artículo\n",
    "article_embeddings = {}\n",
    "\n",
    "for item in embeddings_data:\n",
    "    content_id = item['contentId']\n",
    "    content_token_id = item['content_token_id']\n",
    "    embeddings = item['embeddings']\n",
    "    \n",
    "    if content_id in article_embeddings:\n",
    "        article_embeddings[content_id].append((content_token_id, embeddings))\n",
    "    else:\n",
    "        article_embeddings[content_id] = [(content_token_id, embeddings)]\n",
    "\n",
    "concatenated_embeddings_data = []\n",
    "\n",
    "for content_id, embeddings_list in article_embeddings.items():\n",
    "    # Ordenar los embeddings por content_token_id\n",
    "    embeddings_list.sort(key=lambda x: x[0])\n",
    "    embeddings = [embedding for _, embedding in embeddings_list]\n",
    "    \n",
    "    # Convertir lista de embeddings a tensor\n",
    "    embeddings_tensor = torch.tensor(embeddings)\n",
    "    \n",
    "    # Verificar la forma del tensor antes de concatenar\n",
    "    if embeddings_tensor.ndimension() != 2:\n",
    "        print(f\"Warning: Embeddings for contentId {content_id} have unexpected dimensions: {embeddings_tensor.shape}\")\n",
    "    \n",
    "    # Concatenar los embeddings a lo largo de la primera dimensión\n",
    "    concatenated_embeddings = embeddings_tensor.view(-1).tolist()\n",
    "    concatenated_embeddings_data.append({\n",
    "        'contentId': content_id,\n",
    "        'embeddings': concatenated_embeddings\n",
    "    })\n",
    "\n",
    "# Guardar los embeddings concatenados en un archivo JSON\n",
    "try:\n",
    "    concatenated_embeddings_json = json.dumps(concatenated_embeddings_data)\n",
    "    output_blob_client.upload_blob(concatenated_embeddings_json, blob_type=\"BlockBlob\", overwrite=True)\n",
    "    print(f\"Uploaded concatenated embeddings for {len(concatenated_embeddings_data)} articles to {output_blob_name}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al subir los embeddings concatenated al Blob Storage: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be36db26-341f-4b37-8208-2c1b3b841483",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Downloaded 7414 embeddings fragments.\n",
       "Uploaded first CLS embeddings for 2067 articles to articles_first_cls_embeddings_ALL_roberta.json.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Downloaded 7414 embeddings fragments.\nUploaded first CLS embeddings for 2067 articles to articles_first_cls_embeddings_ALL_roberta.json.\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Técnica First [CLS]\n",
    "\n",
    "# Conectar al Blob Storage\n",
    "blob_service_client = BlobServiceClient.from_connection_string(\"****************\")\n",
    "container_name = \"****************\"\n",
    "embeddings_blob_name = \"articles_fragments_embeddings_roberta.json\"\n",
    "output_blob_name = \"articles_first_cls_embeddings_roberta.json\"\n",
    "embeddings_blob_client = blob_service_client.get_blob_client(container=container_name, blob=embeddings_blob_name)\n",
    "output_blob_client = blob_service_client.get_blob_client(container=container_name, blob=output_blob_name)\n",
    "\n",
    "# Descargar el archivo JSON de embeddings\n",
    "try:\n",
    "    downloaded_blob = embeddings_blob_client.download_blob().readall()\n",
    "    embeddings_data = json.loads(downloaded_blob)\n",
    "    print(f\"Downloaded {len(embeddings_data)} embeddings fragments.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al descargar el archivo JSON: {e}\")\n",
    "    raise\n",
    "\n",
    "# Agrupar embeddings de fragmentos del mismo artículo\n",
    "article_embeddings = {}\n",
    "\n",
    "for item in embeddings_data:\n",
    "    content_id = item['contentId']\n",
    "    content_token_id = item['content_token_id']\n",
    "    embeddings = item['embeddings']\n",
    "    \n",
    "    if content_id in article_embeddings:\n",
    "        article_embeddings[content_id].append((content_token_id, embeddings))\n",
    "    else:\n",
    "        article_embeddings[content_id] = [(content_token_id, embeddings)]\n",
    "\n",
    "first_cls_embeddings_data = []\n",
    "\n",
    "for content_id, embeddings_list in article_embeddings.items():\n",
    "    # Ordenar los embeddings por content_token_id\n",
    "    embeddings_list.sort(key=lambda x: x[0])\n",
    "    first_cls_embedding = embeddings_list[0][1]  # Aquí tomamos el embedding del primer fragmento\n",
    "    first_cls_embeddings_data.append({\n",
    "        'contentId': content_id,\n",
    "        'embedding': first_cls_embedding\n",
    "    })\n",
    "\n",
    "# Guardar los primeros embeddings [CLS] en un archivo JSON\n",
    "try:\n",
    "    first_cls_embeddings_json = json.dumps(first_cls_embeddings_data)\n",
    "    output_blob_client.upload_blob(first_cls_embeddings_json, blob_type=\"BlockBlob\", overwrite=True)\n",
    "    print(f\"Uploaded first CLS embeddings for {len(first_cls_embeddings_data)} articles to {output_blob_name}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al subir los embeddings al Blob Storage: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33b83b11-ff52-4fff-9f78-aa5a9b5a8640",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">tokenizer_config.json:   0%|          | 0.00/25.0 [00:00&lt;?, ?B/s]\n",
       "/databricks/python/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
       "  warnings.warn(\n",
       "config.json:   0%|          | 0.00/482 [00:00&lt;?, ?B/s]\n",
       "vocab.json:   0%|          | 0.00/899k [00:00&lt;?, ?B/s]\n",
       "merges.txt:   0%|          | 0.00/456k [00:00&lt;?, ?B/s]\n",
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00&lt;?, ?B/s]\n",
       "model.safetensors:   0%|          | 0.00/1.42G [00:00&lt;?, ?B/s]\n",
       "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: [&#39;roberta.pooler.dense.bias&#39;, &#39;roberta.pooler.dense.weight&#39;]\n",
       "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
       "Textos que necesitan ser divididos: 0\n",
       "Textos que no necesitan ser divididos: 2097\n",
       "Embeddings de noticias generados y guardados en Azure Blob Storage. Total de fragmentos procesados: 2067\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">tokenizer_config.json:   0%|          | 0.00/25.0 [00:00&lt;?, ?B/s]\n/databricks/python/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nconfig.json:   0%|          | 0.00/482 [00:00&lt;?, ?B/s]\nvocab.json:   0%|          | 0.00/899k [00:00&lt;?, ?B/s]\nmerges.txt:   0%|          | 0.00/456k [00:00&lt;?, ?B/s]\ntokenizer.json:   0%|          | 0.00/1.36M [00:00&lt;?, ?B/s]\nmodel.safetensors:   0%|          | 0.00/1.42G [00:00&lt;?, ?B/s]\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: [&#39;roberta.pooler.dense.bias&#39;, &#39;roberta.pooler.dense.weight&#39;]\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTextos que necesitan ser divididos: 0\nTextos que no necesitan ser divididos: 2097\nEmbeddings de noticias generados y guardados en Azure Blob Storage. Total de fragmentos procesados: 2067\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "import torch\n",
    "\n",
    "# Técnica Sentence-BERT\n",
    "\n",
    "# Configuración de Azure Blob Storage\n",
    "storage_account_name = \"*************\"\n",
    "container_name = \"*************\"\n",
    "storage_account_key = \"*************\"\n",
    "\n",
    "# Crear BlobServiceClient\n",
    "blob_service_client = BlobServiceClient(account_url=f\"https://{storage_account_name}.blob.core.windows.net\", credential=storage_account_key)\n",
    "\n",
    "# Descargar el archivo de artículos del Blob Storage\n",
    "try:\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=\"articles.json\")\n",
    "    articles_data = json.loads(blob_client.download_blob().readall())\n",
    "except Exception as e:\n",
    "    print(f\"Error al descargar el archivo de artículos: {e}\")\n",
    "    raise\n",
    "\n",
    "# Inicializar el modelo de SentenceTransformer\n",
    "model_name = \"roberta-large\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "sentence_transformer_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "# Función para dividir el texto en fragmentos \n",
    "def split_text(text, max_length=4096):\n",
    "    tokens = text.split()\n",
    "    return [' '.join(tokens[i:i + max_length]) for i in range(0, len(tokens), max_length)]\n",
    "\n",
    "# Generar embeddings para las noticias\n",
    "def generate_embeddings(articles):\n",
    "    embeddings = []\n",
    "    needs_split_count = 0\n",
    "    no_split_count = 0\n",
    "    \n",
    "    for article in articles:\n",
    "        text_fragments = split_text(article['text'])\n",
    "        \n",
    "        if len(text_fragments) > 1:\n",
    "            needs_split_count += 1\n",
    "        else:\n",
    "            no_split_count += 1\n",
    "            \n",
    "        for idx, fragment in enumerate(text_fragments):\n",
    "            embedding = sentence_transformer_model.encode(fragment, convert_to_tensor=True)\n",
    "            embeddings.append({\n",
    "                'contentId': article['contentId'],  # Mantener contentId original\n",
    "                'content_token_id': f\"{article['contentId']}_{idx}\",  # Identificar fragmentos\n",
    "                'embedding': embedding.cpu().tolist(),  # Convertir el tensor a lista\n",
    "                'topicName': article['topicName']\n",
    "            })\n",
    "    \n",
    "    print(f\"Textos que necesitan ser divididos: {needs_split_count}\")\n",
    "    print(f\"Textos que no necesitan ser divididos: {no_split_count}\")\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "try:\n",
    "    embeddings = generate_embeddings(articles_data)\n",
    "    # Convertir a JSON\n",
    "    embeddings_json = json.dumps(embeddings, ensure_ascii=False, indent=4)\n",
    "except Exception as e:\n",
    "    print(f\"Error durante la generación de embeddings: {e}\")\n",
    "    raise\n",
    "\n",
    "# Crear BlobClient para los embeddings\n",
    "try:\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=\"articles_embeddings_sbert_roberta.json\")\n",
    "    # Subir los embeddings al Blob Storage\n",
    "    blob_client.upload_blob(embeddings_json, blob_type=\"BlockBlob\", overwrite=True)\n",
    "    print(f\"Embeddings de noticias generados y guardados en Azure Blob Storage. Total de fragmentos procesados: {len(embeddings)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al subir los embeddings al Blob Storage: {e}\")\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BO Embeddings roberta-large",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
